


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_regression
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder





df = pd.read_csv('train.csv')

pd.set_option('display.max_columns', None)

df


df.describe()


df.info()


df.duplicated().value_counts()


df.columns


df = df.drop(columns=['Unnamed: 0'])


columns = df.columns

le=LabelEncoder()

for col in columns : 
    df[col]= le.fit_transform(df[col])

df['class']





plt.figure(figsize=(12,6))
sns.histplot(df['class'], edgecolor='black')
plt.show()


group_df = df.groupby('spore-print-color')['class'].value_counts().reset_index()

sns.barplot(x="spore-print-color", y='count', data=group_df, hue=group_df['class'])
plt.show()


group_df = df.groupby('bruises')['class'].value_counts().reset_index()

bruises_0 = group_df[group_df['bruises'] == 0]
bruises_1 = group_df[group_df['bruises'] == 1]

# Configurar los subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Gráfico para bruises = 0
axes[0].pie(bruises_0['count'], labels=bruises_1['class'],autopct='%1.1f%%', startangle=90, colors=['green', 'purple'])
axes[0].set_title('No es bruise probabilidad de ser venenoso')

# Gráfico para bruises = 1
axes[1].pie(bruises_1['count'], labels=bruises_1['class'],autopct='%1.1f%%', startangle=90, colors=['green', 'purple'])
axes[1].set_title('Es bruise probabilidad de ser venenoso')

# Mostrar los gráficos
plt.tight_layout()
plt.show()





# Correlation matrix

# Seleccionar solo las columnas numéricas

correlation_matr= df.corr().round(1)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()


# Seleccion de features

X_df=df.drop(columns=['class'])
y_df=df['class']

selector = SelectKBest(score_func=f_regression, k=5)
selected_features = selector.fit_transform(X_df, y_df)

print("Selected Features Shape:", selected_features.shape)
selected_feature_names = X_df.columns[selector.get_support()] 


X_df = X_df[selected_feature_names].copy()


# Correlation matrix de los seleccionados

correlation_matr= X_df.corr().round(2)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()



# Calculo del VIF

vif_data = pd.DataFrame()
vif_data['Feature'] = X_df.columns
vif_data['VIF'] = [variance_inflation_factor(X_df.values, i) for i in range(X_df.shape[1])]

print(vif_data)





X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)


from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

# Crear el Pipeline con un paso genérico para el modelo
pipeline = Pipeline([
    ('model', RandomForestClassifier(random_state=42))  # Este será sustituido en el GridSearch
])

# Definir los hiperparámetros para cada modelo
param_grid = [
    {
        'model': [RandomForestClassifier(random_state=42, class_weight="balanced")],
        'model__n_estimators': [10, 50, 150, 200],
        'model__max_depth': [None, 10, 50, 150],
        'model__max_leaf_nodes': [10, 30, 50, 70],
        'model__min_samples_split': [2, 5, 10],
        'model__min_samples_leaf': [1, 2, 3, 4],
        'model__criterion': ['gini', 'entropy']
    },
    {
        'model': [LogisticRegression(random_state=42, max_iter=500)],
        'model__C': [0.1, 1, 10],
        'model__penalty': ['l2'],
        'model__solver': ['lbfgs', 'liblinear']
    },
    {
        'model': [SVC(random_state=42)],
        'model__C': [0.1, 1, 10],
        'model__kernel': ['linear', 'rbf'],
        'model__gamma': ['scale', 'auto']
    },
    {
        'model': [DecisionTreeClassifier(random_state=42)],
        'model__max_depth': [10, 50, 150, 200],
        'model__max_leaf_nodes': [10, 30, 50, 70],
        'model__min_samples_split': [2, 5, 10],
        'model__min_samples_leaf': [1, 2, 3, 4],
        'model__criterion': ['gini', 'entropy']
    }
]

# Configurar el GridSearchCV
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,  # Validación cruzada
    scoring='accuracy',  # Métrica a optimizar
    verbose=0,
    n_jobs=-1  # Usar todos los núcleos disponibles
)

# Ejecutar el GridSearch
grid_search.fit(X_train, y_train)

# Mostrar los mejores hiperparámetros y el modelo ganador
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
print("Best Model:", grid_search.best_estimator_)





df_test = pd.read_csv('test.csv')

columns = df_test.columns

le=LabelEncoder()

for col in columns : 
    df_test[col]= le.fit_transform(df_test[col])

df_test = df_test[[col for col in X_df.columns if col in df_test.columns]]

df_test





y_pred = grid_search.predict(df_test)


predict_csv = pd.DataFrame()
predict_csv['ID']= 0
predict_csv['class']= y_pred
predict_csv['ID']= predict_csv.index+1

predict_csv['class'] = ['p' if val == 1 else 'e' for val in y_pred]

predict_csv


predict_csv.to_csv('predict.csv', index=False)
